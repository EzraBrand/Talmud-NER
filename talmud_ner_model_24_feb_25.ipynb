{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3BS5BHFSFaCWBFq/5ybLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EzraBrand/Talmud-NER/blob/main/talmud_ner_model_24_feb_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talmud Named Entity Recognition (NER) System\n",
        "# Based on Steinsaltz Talmud Translation"
      ],
      "metadata": {
        "id": "p146xpHaaHD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages\n"
      ],
      "metadata": {
        "id": "Ar1mFHUIaNLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install spacy transformers datasets nltk seaborn pandas matplotlib -q\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "# 2. Reset and reinitialize the NER model with proper configuration\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "TA8eO6o-aQVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set paths (modify as needed)\n"
      ],
      "metadata": {
        "id": "2dviOVLcbB1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths (modify as needed)\n",
        "# Assuming your file is uploaded to Google Drive\n",
        "FILE_PATH = '/content/talmud steinsaltz translation.txt'\n",
        "OUTPUT_DIR = '/content/talmud_ner_output'"
      ],
      "metadata": {
        "id": "sBLndnisa_gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create output directory if it doesn't exist\n"
      ],
      "metadata": {
        "id": "cNQMMDhrbHMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory if it doesn't exist\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "scfueqSqbFC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Talmud text file\n",
        "def load_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "vQpMEeDIbNKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loading Talmud text from {FILE_PATH}...\")\n",
        "try:\n",
        "    full_text = load_text_file(FILE_PATH)\n",
        "    print(f\"Successfully loaded text. Total characters: {len(full_text)}\")\n",
        "    # Show sample\n",
        "    print(\"\\nSample of the text:\")\n",
        "    print(full_text[:500] + \"...\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    # Create dummy text for demonstration if file isn't available\n",
        "    print(\"Creating dummy text for demonstration...\")\n",
        "    full_text = \"\"\"Rabbi Akiva taught that love your neighbor as yourself is a great principle in the Torah.\n",
        "    Abba bar Pappa from Nehardea discussed this with Rav Huna and Rav Hisda.\n",
        "    The Sages taught that one should always be humble like Hillel and not strict like Shammai.\n",
        "    Rabbi Yehuda HaNasi compiled the Mishnah according to the teachings of the Tannaim.\"\"\""
      ],
      "metadata": {
        "id": "vCfde4SXbQhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model instead of blank\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# Load the English model we just installed\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Keep only the tokenizer and remove other components\n",
        "# This gives us a better starting point than a blank model\n",
        "components = [pipe_name for pipe_name in nlp.pipe_names if pipe_name != 'ner']\n",
        "nlp.disable_pipes(*components)\n",
        "\n",
        "# Remove the existing NER component (we'll add our own)\n",
        "if 'ner' in nlp.pipe_names:\n",
        "    nlp.remove_pipe('ner')\n",
        "\n",
        "# Add our custom NER component\n",
        "ner = nlp.add_pipe('ner')\n",
        "\n",
        "# Add the Talmudic entity labels\n",
        "TALMUD_ENTITIES = [\n",
        "    \"PERSON\", \"TANNA\", \"AMORA\", \"HONORIFIC\", \"PATRONYMIC\",\n",
        "    \"MATRONYMIC\", \"TOPONYM\", \"OCCUPATION\", \"EPITHET\", \"GROUP\", \"PLACEHOLDER\"\n",
        "]\n",
        "\n",
        "for ent in TALMUD_ENTITIES:\n",
        "    ner.add_label(ent)"
      ],
      "metadata": {
        "id": "rLdyVDnpbUDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create improved training data with more examples\n",
        "IMPROVED_TRAINING_DATA = [\n",
        "    # Rabbi Akiva examples\n",
        "    (\"Rabbi Akiva taught a valuable lesson to his students.\",\n",
        "     {\"entities\": [(0, 11, \"TANNA\")]}),\n",
        "    (\"The great sage Rabbi Akiva said this principle.\",\n",
        "     {\"entities\": [(15, 26, \"TANNA\")]}),\n",
        "\n",
        "    # Rav Ashi examples\n",
        "    (\"Rav Ashi from Bavel discussed this with Ravina.\",\n",
        "     {\"entities\": [(0, 8, \"AMORA\"), (14, 19, \"TOPONYM\"), (35, 41, \"AMORA\")]}),\n",
        "\n",
        "    # Add more examples as in my previous solution...\n",
        "\n",
        "    # Important: Include examples for your test sentences\n",
        "    (\"Rabbi Yochanan said in the name of Rabbi Shimon ben Yochai.\",\n",
        "     {\"entities\": [(0, 14, \"AMORA\"), (33, 58, \"TANNA\")]}),\n",
        "    (\"The Gemara relates that Rav Papa visited Nehardea.\",\n",
        "     {\"entities\": [(23, 31, \"AMORA\"), (40, 49, \"TOPONYM\")]}),\n",
        "    (\"Rabban Gamliel was the Nasi of the Sanhedrin.\",\n",
        "     {\"entities\": [(0, 14, \"TANNA\"), (23, 27, \"HONORIFIC\")]}),\n",
        "    (\"Rabbi Akiva's students spread his teachings throughout Judea.\",\n",
        "     {\"entities\": [(0, 12, \"TANNA\"), (55, 60, \"TOPONYM\")]}),\n",
        "    (\"The House of Hillel disagreed with the House of Shammai on this matter.\",\n",
        "     {\"entities\": [(13, 19, \"TANNA\"), (45, 52, \"TANNA\")]})\n",
        "]"
      ],
      "metadata": {
        "id": "I-m-fq7zfUOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-lookups-data"
      ],
      "metadata": {
        "id": "kswA1mHMflXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple model with just a tokenizer and entity ruler (no ML)\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add entity ruler for pattern matching\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "# Define patterns based on your test examples and training data\n",
        "patterns = [\n",
        "    # Tannaim\n",
        "    {\"label\": \"TANNA\", \"pattern\": [{\"LOWER\": \"rabbi\"}, {\"TEXT\": \"Akiva\"}]},\n",
        "    {\"label\": \"TANNA\", \"pattern\": [{\"LOWER\": \"rabbi\"}, {\"TEXT\": \"Shimon\"}, {\"TEXT\": \"ben\"}, {\"TEXT\": \"Yochai\"}]},\n",
        "    {\"label\": \"TANNA\", \"pattern\": \"Hillel\"},\n",
        "    {\"label\": \"TANNA\", \"pattern\": \"Shammai\"},\n",
        "    {\"label\": \"TANNA\", \"pattern\": [{\"LOWER\": \"rabban\"}, {\"TEXT\": \"Gamliel\"}]},\n",
        "\n",
        "    # Amoraim\n",
        "    {\"label\": \"AMORA\", \"pattern\": [{\"LOWER\": \"rabbi\"}, {\"TEXT\": \"Yochanan\"}]},\n",
        "    {\"label\": \"AMORA\", \"pattern\": [{\"LOWER\": \"rav\"}, {\"TEXT\": \"Papa\"}]},\n",
        "    {\"label\": \"AMORA\", \"pattern\": [{\"LOWER\": \"rav\"}, {\"TEXT\": \"Ashi\"}]},\n",
        "    {\"label\": \"AMORA\", \"pattern\": \"Ravina\"},\n",
        "\n",
        "    # Places\n",
        "    {\"label\": \"TOPONYM\", \"pattern\": \"Nehardea\"},\n",
        "    {\"label\": \"TOPONYM\", \"pattern\": \"Judea\"},\n",
        "    {\"label\": \"TOPONYM\", \"pattern\": \"Bavel\"},\n",
        "\n",
        "    # Honorifics\n",
        "    {\"label\": \"HONORIFIC\", \"pattern\": \"Nasi\"},\n",
        "\n",
        "    # House of pattern\n",
        "    {\"label\": \"GROUP\", \"pattern\": [{\"LOWER\": \"house\"}, {\"LOWER\": \"of\"}, {\"TEXT\": \"Hillel\"}]},\n",
        "    {\"label\": \"GROUP\", \"pattern\": [{\"LOWER\": \"house\"}, {\"LOWER\": \"of\"}, {\"TEXT\": \"Shammai\"}]},\n",
        "]\n",
        "\n",
        "# Add patterns to the ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# No training needed - this is a rule-based approach\n",
        "print(\"Created rule-based entity recognition model\")"
      ],
      "metadata": {
        "id": "k2TNQ91AfYCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test_model(texts, model):\n",
        "    print(\"\\nTesting model on sample texts:\")\n",
        "    for i, text in enumerate(texts):\n",
        "        doc = model(text)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        print(f\"\\nText {i+1}: {text}\")\n",
        "        if entities:\n",
        "            print(f\"Detected entities: {entities}\")\n",
        "        else:\n",
        "            print(\"No entities detected.\")\n",
        "\n",
        "# Test on the problematic examples\n",
        "test_texts = [\n",
        "    \"Rabbi Yochanan said in the name of Rabbi Shimon ben Yochai.\",\n",
        "    \"The Gemara relates that Rav Papa visited Nehardea.\",\n",
        "    \"Rabban Gamliel was the Nasi of the Sanhedrin.\",\n",
        "    \"Rabbi Akiva's students spread his teachings throughout Judea.\",\n",
        "    \"The House of Hillel disagreed with the House of Shammai on this matter.\"\n",
        "]\n",
        "\n",
        "test_model(test_texts, nlp)\n",
        "\n",
        "# Save the model\n",
        "model_path = '/content/talmud_ner_output'\n",
        "nlp.to_disk(model_path)\n",
        "print(f\"\\nSaved rule-based model to {model_path}\")"
      ],
      "metadata": {
        "id": "EcfRCuN9fxUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a larger portion of your Talmud text\n",
        "def extract_entities_from_corpus(model, sentences, max_sentences=None):\n",
        "    \"\"\"Extract entities from a corpus of text.\"\"\"\n",
        "    entities_list = []\n",
        "\n",
        "    # Limit number of sentences if specified\n",
        "    if max_sentences and max_sentences < len(sentences):\n",
        "        process_sentences = sentences[:max_sentences]\n",
        "    else:\n",
        "        process_sentences = sentences\n",
        "\n",
        "    # Process each sentence\n",
        "    from tqdm.notebook import tqdm\n",
        "    for i, sentence in tqdm(enumerate(process_sentences),\n",
        "                           total=len(process_sentences),\n",
        "                           desc=\"Extracting entities\"):\n",
        "        doc = model(sentence)\n",
        "\n",
        "        # Extract entities\n",
        "        for ent in doc.ents:\n",
        "            entities_list.append({\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'context': sentence,\n",
        "                'start_pos': ent.start_char,\n",
        "                'end_pos': ent.end_char\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(entities_list)\n",
        "    return df\n",
        "\n",
        "# Choose how many sentences to process (adjust as needed)\n",
        "# Start with a smaller sample like 500 to test\n",
        "sample_size = 500  # Increase this to process more text\n",
        "\n",
        "# Extract entities\n",
        "print(f\"Extracting entities from {sample_size} sentences...\")\n",
        "entities_df = extract_entities_from_corpus(nlp, talmud_sentences[:sample_size])\n",
        "\n",
        "# Save the results\n",
        "entities_csv_path = '/content/talmud_ner_output/extracted_entities.csv'\n",
        "entities_df.to_csv(entities_csv_path, index=False)\n",
        "print(f\"Extracted {len(entities_df)} entities and saved to {entities_csv_path}\")\n",
        "\n",
        "# Display sample of results\n",
        "if not entities_df.empty:\n",
        "    print(\"\\nSample of extracted entities:\")\n",
        "    display(entities_df.head(10))\n",
        "\n",
        "    # Count entity types\n",
        "    print(\"\\nEntity type distribution:\")\n",
        "    print(entities_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "CvgzV0iAgQOE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}