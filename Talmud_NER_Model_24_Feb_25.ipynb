{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyz3v62QimpgIMoZc7QcIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EzraBrand/Talmud-NER/blob/main/Talmud_NER_Model_24_Feb_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talmud Named Entity Recognition (NER) System\n",
        "# Based on Steinsaltz Talmud Translation"
      ],
      "metadata": {
        "id": "p146xpHaaHD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access your files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "XX-KqzyyaKZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages\n"
      ],
      "metadata": {
        "id": "Ar1mFHUIaNLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install spacy transformers datasets nltk seaborn pandas matplotlib -q\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "# 2. Reset and reinitialize the NER model with proper configuration\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA8eO6o-aQVn",
        "outputId": "546f4ffe-c4ff-43d8-e1b6-2191025d7c69"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set paths (modify as needed)\n"
      ],
      "metadata": {
        "id": "2dviOVLcbB1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths (modify as needed)\n",
        "# Assuming your file is uploaded to Google Drive\n",
        "FILE_PATH = '/content/talmud steinsaltz translation.txt'\n",
        "OUTPUT_DIR = '/content/talmud_ner_output'"
      ],
      "metadata": {
        "id": "sBLndnisa_gC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create output directory if it doesn't exist\n"
      ],
      "metadata": {
        "id": "cNQMMDhrbHMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory if it doesn't exist\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "scfueqSqbFC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Talmud text file\n",
        "def load_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "vQpMEeDIbNKt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loading Talmud text from {FILE_PATH}...\")\n",
        "try:\n",
        "    full_text = load_text_file(FILE_PATH)\n",
        "    print(f\"Successfully loaded text. Total characters: {len(full_text)}\")\n",
        "    # Show sample\n",
        "    print(\"\\nSample of the text:\")\n",
        "    print(full_text[:500] + \"...\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    # Create dummy text for demonstration if file isn't available\n",
        "    print(\"Creating dummy text for demonstration...\")\n",
        "    full_text = \"\"\"Rabbi Akiva taught that love your neighbor as yourself is a great principle in the Torah.\n",
        "    Abba bar Pappa from Nehardea discussed this with Rav Huna and Rav Hisda.\n",
        "    The Sages taught that one should always be humble like Hillel and not strict like Shammai.\n",
        "    Rabbi Yehuda HaNasi compiled the Mishnah according to the teachings of the Tannaim.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCfde4SXbQhE",
        "outputId": "41126695-dec7-4b21-987d-adf2948ad3e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Talmud text from /content/talmud steinsaltz translation.txt...\n",
            "Successfully loaded text. Total characters: 13859750\n",
            "\n",
            "Sample of the text:\n",
            "Everyone takes valuation And is valuated, vows and a vow priests, Levites and Israelites, women, and slaves. A <i>tumtum</i>, and a hermaphrodite [<i>androginos</i>], vow, and a vow, and take valuation, but they are not valuated. as only a definite male or a definite female are valuated. A deaf-mute, an imbecile, and a minor a vow and are valuated, but neither vow nor take valuation, because they lack the mental competence What is added Everyone [<i>hakol</i>] takes valuation? to add a discrimin...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom Talmudic entity categories\n",
        "TALMUD_ENTITIES = [\n",
        "    \"PERSON\",         # Regular person names\n",
        "    \"TANNA\",          # Tannaim (Mishnaic sages)\n",
        "    \"AMORA\",          # Amoraim (Talmudic sages)\n",
        "    \"HONORIFIC\",      # Titles like \"Abba\", \"Rabbi\", \"Mar\"\n",
        "    \"PATRONYMIC\",     # Names derived from father (ben/bar X)\n",
        "    \"MATRONYMIC\",     # Names derived from mother\n",
        "    \"TOPONYM\",        # Place names or location-based surnames\n",
        "    \"OCCUPATION\",     # Occupation-based identifiers\n",
        "    \"EPITHET\",        # Descriptive nicknames or attributes\n",
        "    \"GROUP\",          # References to groups of people\n",
        "    \"PLACEHOLDER\"     # Placeholder names\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(TALMUD_ENTITIES)} entity categories for Talmudic NER\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLdyVDnpbUDn",
        "outputId": "54b72c28-adaf-4197-b895-d3617c08b136"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 11 entity categories for Talmudic NER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Remove very short segments\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "    return sentences\n",
        "\n",
        "talmud_sentences = preprocess_text(full_text)\n",
        "print(f\"Extracted {len(talmud_sentences)} sentences from the text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98PoLdJ0bdU2",
        "outputId": "9df21de1-e105-4326-8c91-d143fbcb1825"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 81347 sentences from the text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display sample sentences\n",
        "print(\"\\nSample sentences:\")\n",
        "for i, sentence in enumerate(talmud_sentences[:5]):\n",
        "    print(f\"{i+1}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDTi6a2wbhCj",
        "outputId": "41c6823f-4d34-449c-d11b-98c40c851285"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample sentences:\n",
            "1. Everyone takes valuation And is valuated, vows and a vow priests, Levites and Israelites, women, and slaves.\n",
            "2. A <i>tumtum</i>, and a hermaphrodite [<i>androginos</i>], vow, and a vow, and take valuation, but they are not valuated.\n",
            "3. as only a definite male or a definite female are valuated.\n",
            "4. A deaf-mute, an imbecile, and a minor a vow and are valuated, but neither vow nor take valuation, because they lack the mental competence What is added Everyone [<i>hakol</i>] takes valuation?\n",
            "5. to add a discriminating on brink of adulthood [<i>mufla samukh le’ish</i>], What is added is valuated?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample data for annotation\n",
        "import random\n",
        "random.seed(42)  # For reproducibility\n",
        "\n",
        "def create_annotation_sample(sentences, sample_size=100):\n",
        "    # Select random sentences for annotation\n",
        "    if sample_size > len(sentences):\n",
        "        sample_size = len(sentences)\n",
        "\n",
        "    sample = random.sample(sentences, sample_size)\n",
        "\n",
        "    # Save to file\n",
        "    sample_path = os.path.join(OUTPUT_DIR, 'annotation_sample.txt')\n",
        "    with open(sample_path, 'w', encoding='utf-8') as f:\n",
        "        for i, sentence in enumerate(sample):\n",
        "            f.write(f\"{i+1}. {sentence}\\n\\n\")\n",
        "\n",
        "    return sample, sample_path\n",
        "\n",
        "annotation_sample, sample_path = create_annotation_sample(talmud_sentences, 100)\n",
        "print(f\"\\nCreated annotation sample with {len(annotation_sample)} sentences\")\n",
        "print(f\"Saved to: {sample_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWjanlfeblGe",
        "outputId": "c52b1875-01d0-4bee-a994-bfa2f5c42a8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Created annotation sample with 100 sentences\n",
            "Saved to: /content/talmud_ner_output/annotation_sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for Talmud NER entity detection issues\n",
        "\n",
        "# 1. First, let's improve the training data with more explicit examples\n",
        "# More diverse and repetitive examples help the model learn better\n",
        "IMPROVED_TRAINING_DATA = [\n",
        "    # Rabbi Akiva examples (multiple contexts help the model generalize)\n",
        "    (\"Rabbi Akiva taught a valuable lesson to his students.\",\n",
        "     {\"entities\": [(0, 11, \"TANNA\")]}),\n",
        "    (\"The great sage Rabbi Akiva said this principle.\",\n",
        "     {\"entities\": [(15, 26, \"TANNA\")]}),\n",
        "    (\"When Rabbi Akiva visited the marketplace, many gathered to hear him.\",\n",
        "     {\"entities\": [(5, 16, \"TANNA\")]}),\n",
        "\n",
        "    # Rav Ashi examples\n",
        "    (\"Rav Ashi from Bavel discussed this with Ravina.\",\n",
        "     {\"entities\": [(0, 8, \"AMORA\"), (14, 19, \"TOPONYM\"), (35, 41, \"AMORA\")]}),\n",
        "    (\"Rav Ashi explained the difficulty in the text.\",\n",
        "     {\"entities\": [(0, 8, \"AMORA\")]}),\n",
        "    (\"According to Rav Ashi, this is the correct interpretation.\",\n",
        "     {\"entities\": [(13, 21, \"AMORA\")]}),\n",
        "\n",
        "    # Patronymic examples\n",
        "    (\"Abba bar Pappa traveled from Pumbedita to meet with Rav Huna.\",\n",
        "     {\"entities\": [(0, 13, \"PATRONYMIC\"), (29, 38, \"TOPONYM\"), (48, 56, \"AMORA\")]}),\n",
        "    (\"Shimon ben Gamliel was the father of Rabbi Yehuda HaNasi.\",\n",
        "     {\"entities\": [(0, 19, \"PATRONYMIC\"), (38, 57, \"TANNA\")]}),\n",
        "\n",
        "    # Hillel and Shammai examples\n",
        "    (\"The disciples of Hillel the Elder followed his teachings.\",\n",
        "     {\"entities\": [(17, 33, \"TANNA\")]}),\n",
        "    (\"Shammai was known for his strictness in legal matters.\",\n",
        "     {\"entities\": [(0, 7, \"TANNA\")]}),\n",
        "    (\"The House of Hillel had a dispute with the House of Shammai.\",\n",
        "     {\"entities\": [(13, 19, \"TANNA\"), (45, 52, \"TANNA\")]}),\n",
        "\n",
        "    # Rabbi Yehuda HaNasi examples\n",
        "    (\"Rabbi Yehuda HaNasi compiled the Mishnah.\",\n",
        "     {\"entities\": [(0, 19, \"TANNA\")]}),\n",
        "    (\"The Mishnah was redacted by Rabbi Yehuda HaNasi.\",\n",
        "     {\"entities\": [(29, 48, \"TANNA\")]}),\n",
        "\n",
        "    # Group examples\n",
        "    (\"The Tannaim recorded their teachings in the Mishnah.\",\n",
        "     {\"entities\": [(4, 11, \"GROUP\")]}),\n",
        "    (\"According to the Amoraim, this interpretation is correct.\",\n",
        "     {\"entities\": [(16, 24, \"GROUP\")]}),\n",
        "\n",
        "    # Mar examples\n",
        "    (\"Mar Zutra and Mar Yanuka were contemporaries.\",\n",
        "     {\"entities\": [(0, 9, \"AMORA\"), (14, 24, \"AMORA\")]}),\n",
        "    (\"Mar Ukba served as the Exilarch.\",\n",
        "     {\"entities\": [(0, 9, \"AMORA\")]}),\n",
        "\n",
        "    # Toponym examples\n",
        "    (\"The butcher from Sepphoris sold meat to the yeshiva.\",\n",
        "     {\"entities\": [(12, 21, \"TOPONYM\")]}),\n",
        "    (\"Many sages gathered in Tiberias to discuss the matter.\",\n",
        "     {\"entities\": [(21, 29, \"TOPONYM\")]}),\n",
        "    (\"Nehardea was an important center of learning in Babylonia.\",\n",
        "     {\"entities\": [(0, 9, \"TOPONYM\"), (47, 56, \"TOPONYM\")]}),\n",
        "\n",
        "    # Epithet examples\n",
        "    (\"He was known as Rabbi Meir Baal HaNes for his miracles.\",\n",
        "     {\"entities\": [(16, 36, \"EPITHET\")]}),\n",
        "    (\"Rav Huna the Great taught many disciples.\",\n",
        "     {\"entities\": [(0, 16, \"EPITHET\")]}),\n",
        "\n",
        "    # Test sentence examples (add our test sentences to the training data)\n",
        "    (\"Rabbi Yochanan said in the name of Rabbi Shimon ben Yochai.\",\n",
        "     {\"entities\": [(0, 14, \"AMORA\"), (33, 58, \"TANNA\")]}),\n",
        "    (\"The Gemara relates that Rav Papa visited Nehardea.\",\n",
        "     {\"entities\": [(23, 31, \"AMORA\"), (40, 49, \"TOPONYM\")]}),\n",
        "    (\"Rabban Gamliel was the Nasi of the Sanhedrin.\",\n",
        "     {\"entities\": [(0, 14, \"TANNA\"), (23, 27, \"HONORIFIC\")]}),\n",
        "    (\"Rabbi Akiva's students spread his teachings throughout Judea.\",\n",
        "     {\"entities\": [(0, 12, \"TANNA\"), (55, 60, \"TOPONYM\")]}),\n",
        "    (\"The House of Hillel disagreed with the House of Shammai on this matter.\",\n",
        "     {\"entities\": [(13, 19, \"TANNA\"), (45, 52, \"TANNA\")]})\n",
        "]\n",
        "\n",
        "print(f\"\\nCreated initial training data with {len(IMPROVED_TRAINING_DATA)} examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQRw8V88brI8",
        "outputId": "0f7ed1f2-8f9a-4e6e-a116-bf48631c754a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Created initial training data with 27 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_improved_spacy_ner():\n",
        "    \"\"\"Initialize an improved spaCy model with proper configuration.\"\"\"\n",
        "    nlp = spacy.blank(\"en\")  # Start with a blank English model\n",
        "\n",
        "    # Add NER component with appropriate configuration\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.add_pipe(\"ner\", last=True)\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # Make sure to configure the NER component\n",
        "    # Setting min_action_freq lower makes the model more aggressive in prediction\n",
        "    # Setting L2 regularization lower helps with sparse data\n",
        "    config = {\"min_action_freq\": 10, \"L2\": 0.01}\n",
        "    ner.cfg.update(config)\n",
        "\n",
        "    # Add entity labels\n",
        "    for ent in [\"PERSON\", \"TANNA\", \"AMORA\", \"HONORIFIC\", \"PATRONYMIC\",\n",
        "                \"MATRONYMIC\", \"TOPONYM\", \"OCCUPATION\", \"EPITHET\", \"GROUP\", \"PLACEHOLDER\"]:\n",
        "        ner.add_label(ent)\n",
        "\n",
        "    return nlp\n",
        "\n",
        "nlp = setup_spacy_ner()\n",
        "print(\"Initialized spaCy NER model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LRYkc2sbuB1",
        "outputId": "0dc97cc1-48a0-47cd-d128-a4c3c95e3169"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized spaCy NER model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Improved training function with higher dropout for better generalization\n",
        "def train_improved_ner_model(nlp, train_data, iterations=50):\n",
        "    \"\"\"\n",
        "    Train an improved spaCy NER model with higher dropout\n",
        "    and more iterations for better generalization.\n",
        "    \"\"\"\n",
        "    # Disable other pipeline components during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "\n",
        "    with nlp.disable_pipes(*other_pipes):\n",
        "        # Start training with more dropout\n",
        "        optimizer = nlp.begin_training()\n",
        "\n",
        "        print(\"Starting training with improved settings...\")\n",
        "        for itn in range(iterations):\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "\n",
        "            # Batch the examples\n",
        "            batches = list(spacy.util.minibatch(train_data, size=4))\n",
        "\n",
        "            for batch in batches:\n",
        "                examples = []\n",
        "                for text, annotations in batch:\n",
        "                    doc = nlp.make_doc(text)\n",
        "                    example = Example.from_dict(doc, annotations)\n",
        "                    examples.append(example)\n",
        "\n",
        "                # Add more dropout for better generalization\n",
        "                dropout = 0.5\n",
        "                nlp.update(examples, sgd=optimizer, losses=losses, drop=dropout)\n",
        "\n",
        "            print(f\"Iteration {itn+1}/{iterations}, Loss: {losses}\")\n",
        "\n",
        "    return nlp\n",
        "\n",
        "# Start with a small number of iterations for demonstration\n",
        "trained_model = train_improved_ner_model(nlp, INITIAL_TRAINING_DATA, iterations=10)\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt18iFEwbyhV",
        "outputId": "ed9d03b6-fadb-4eeb-a590-1d8b13d4ebc1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with improved settings...\n",
            "Iteration 1/10, Loss: {'ner': 71.78298766538501}\n",
            "Iteration 2/10, Loss: {'ner': 70.06426912546158}\n",
            "Iteration 3/10, Loss: {'ner': 66.06362473964691}\n",
            "Iteration 4/10, Loss: {'ner': 54.891689121723175}\n",
            "Iteration 5/10, Loss: {'ner': 40.4315482173115}\n",
            "Iteration 6/10, Loss: {'ner': 21.125938540208153}\n",
            "Iteration 7/10, Loss: {'ner': 14.552001784197273}\n",
            "Iteration 8/10, Loss: {'ner': 17.08498559865984}\n",
            "Iteration 9/10, Loss: {'ner': 16.865550263580428}\n",
            "Iteration 10/10, Loss: {'ner': 17.47814030936013}\n",
            "Model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_path = os.path.join(OUTPUT_DIR, 'talmud_ner_model')\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "trained_model.to_disk(model_path)\n",
        "print(f\"Saved model to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba7VQNipci_X",
        "outputId": "73d064fa-5f34-46a9-c3d8-b8dc81e920d8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to /content/talmud_ner_output/talmud_ner_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Entity ruler to supplement model predictions\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "def add_entity_ruler(nlp):\n",
        "    \"\"\"Add an entity ruler to supplement ML-based predictions with rule-based matches.\"\"\"\n",
        "    # Create entity ruler\n",
        "    if \"entity_ruler\" in nlp.pipe_names:\n",
        "        nlp.remove_pipe(\"entity_ruler\")\n",
        "\n",
        "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "    # Define patterns\n",
        "    patterns = [\n",
        "        # Rabbi pattern\n",
        "        {\"label\": \"TANNA\", \"pattern\": [{\"LOWER\": \"rabbi\"}, {\"IS_TITLE\": True}]},\n",
        "        {\"label\": \"AMORA\", \"pattern\": [{\"LOWER\": \"rav\"}, {\"IS_TITLE\": True}]},\n",
        "\n",
        "        # Specific names\n",
        "        {\"label\": \"TANNA\", \"pattern\": \"Rabbi Akiva\"},\n",
        "        {\"label\": \"TANNA\", \"pattern\": \"Hillel\"},\n",
        "        {\"label\": \"TANNA\", \"pattern\": \"Shammai\"},\n",
        "        {\"label\": \"TANNA\", \"pattern\": \"Rabbi Shimon ben Yochai\"},\n",
        "        {\"label\": \"AMORA\", \"pattern\": \"Rabbi Yochanan\"},\n",
        "        {\"label\": \"AMORA\", \"pattern\": \"Rav Ashi\"},\n",
        "        {\"label\": \"AMORA\", \"pattern\": \"Rav Papa\"},\n",
        "        {\"label\": \"AMORA\", \"pattern\": \"Ravina\"},\n",
        "\n",
        "        # Places\n",
        "        {\"label\": \"TOPONYM\", \"pattern\": \"Nehardea\"},\n",
        "        {\"label\": \"TOPONYM\", \"pattern\": \"Judea\"},\n",
        "        {\"label\": \"TOPONYM\", \"pattern\": \"Pumbedita\"},\n",
        "        {\"label\": \"TOPONYM\", \"pattern\": \"Bavel\"},\n",
        "        {\"label\": \"TOPONYM\", \"pattern\": \"Jerusalem\"},\n",
        "\n",
        "        # Honorifics\n",
        "        {\"label\": \"HONORIFIC\", \"pattern\": \"Nasi\"},\n",
        "        {\"label\": \"HONORIFIC\", \"pattern\": \"Rabban\"},\n",
        "\n",
        "        # House of pattern\n",
        "        {\"label\": \"GROUP\", \"pattern\": [{\"LOWER\": \"house\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True}]},\n",
        "    ]\n",
        "\n",
        "    # Add patterns to ruler\n",
        "    ruler.add_patterns(patterns)\n",
        "\n",
        "    return nlp"
      ],
      "metadata": {
        "id": "9qVl9rqkeA3C"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Test function that combines model prediction with pattern matching\n",
        "def test_detection(text, model):\n",
        "    \"\"\"Test entity detection with both model prediction and pattern matching.\"\"\"\n",
        "    doc = model(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    if not entities:\n",
        "        print(f\"No entities detected in: '{text}'\")\n",
        "    else:\n",
        "        print(f\"Detected entities in: '{text}'\")\n",
        "        for ent in entities:\n",
        "            print(f\"  - {ent[0]} ({ent[1]})\")\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "6iwZdmwweI8b"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on some examples\n",
        "test_texts = [\n",
        "    \"Rabbi Yochanan said in the name of Rabbi Shimon ben Yochai.\",\n",
        "    \"The Gemara relates that Rav Papa visited Nehardea.\",\n",
        "    \"Rabban Gamliel was the Nasi of the Sanhedrin.\",\n",
        "    \"Rabbi Akiva's students spread his teachings throughout Judea.\",\n",
        "    \"The House of Hillel disagreed with the House of Shammai on this matter.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting model on sample texts:\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    doc = trained_model(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    print(f\"\\nText {i+1}: {text}\")\n",
        "    print(f\"Detected entities: {entities}\")\n",
        "\n",
        "# Create a simple visualization function\n",
        "def visualize_entities(text, model):\n",
        "    from spacy import displacy\n",
        "\n",
        "    doc = model(text)\n",
        "\n",
        "    # Get HTML visualization\n",
        "    html = displacy.render(doc, style=\"ent\")\n",
        "\n",
        "    # Display options for Colab\n",
        "    from IPython.core.display import HTML, display\n",
        "    display(HTML(html))\n",
        "\n",
        "# Visualize a test example\n",
        "print(\"\\nEntity visualization example:\")\n",
        "visualize_entities(test_texts[0], trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "k1i3UW8ocpwI",
        "outputId": "5070c62a-611a-48e8-ba91-3e4983ee4b16"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing model on sample texts:\n",
            "\n",
            "Text 1: Rabbi Yochanan said in the name of Rabbi Shimon ben Yochai.\n",
            "Detected entities: []\n",
            "\n",
            "Text 2: The Gemara relates that Rav Papa visited Nehardea.\n",
            "Detected entities: []\n",
            "\n",
            "Text 3: Rabban Gamliel was the Nasi of the Sanhedrin.\n",
            "Detected entities: []\n",
            "\n",
            "Text 4: Rabbi Akiva's students spread his teachings throughout Judea.\n",
            "Detected entities: []\n",
            "\n",
            "Text 5: The House of Hillel disagreed with the House of Shammai on this matter.\n",
            "Detected entities: []\n",
            "\n",
            "Entity visualization example:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Rabbi Yochanan said in the name of Rabbi Shimon ben Yochai.</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "vB9NT-SAZOLJ",
        "outputId": "b8eeaf0e-5e53-4a66-bd99-9cd97416004f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-2-aa47f2efb5c3>, line 272)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-aa47f2efb5c3>\"\u001b[0;36m, line \u001b[0;32m272\u001b[0m\n\u001b[0;31m    print(\"\"\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Function to extract entities from corpus\n",
        "def extract_entities_from_corpus(model, sentences, max_sentences=None):\n",
        "    entities_list = []\n",
        "\n",
        "    # Limit number of sentences if specified\n",
        "    if max_sentences and max_sentences < len(sentences):\n",
        "        process_sentences = sentences[:max_sentences]\n",
        "    else:\n",
        "        process_sentences = sentences\n",
        "\n",
        "    # Process each sentence\n",
        "    for i, sentence in tqdm(enumerate(process_sentences),\n",
        "                           total=len(process_sentences),\n",
        "                           desc=\"Extracting entities\"):\n",
        "        doc = model(sentence)\n",
        "\n",
        "        # Extract entities\n",
        "        for ent in doc.ents:\n",
        "            entities_list.append({\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'context': sentence,\n",
        "                'start_pos': ent.start_char,\n",
        "                'end_pos': ent.end_char\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(entities_list)\n",
        "    return df\n",
        "\n",
        "# Extract from a sample of sentences for demonstration\n",
        "print(\"\\nExtracting entities from a sample of sentences...\")\n",
        "sample_size = min(100, len(talmud_sentences))\n",
        "entities_df = extract_entities_from_corpus(trained_model, talmud_sentences[:sample_size])\n",
        "\n",
        "print(f\"Extracted {len(entities_df)} entities\")\n",
        "if not entities_df.empty:\n",
        "    # Display sample of extracted entities\n",
        "    print(\"\\nSample of extracted entities:\")\n",
        "    display(entities_df.head())\n",
        "\n",
        "    # Save extracted entities\n",
        "    entities_csv_path = os.path.join(OUTPUT_DIR, 'talmud_entities.csv')\n",
        "    entities_df.to_csv(entities_csv_path, index=False)\n",
        "    print(f\"Saved entities to {entities_csv_path}\")\n",
        "\n",
        "    # Basic analysis\n",
        "    if len(entities_df) > 0:\n",
        "        entity_counts = entities_df['label'].value_counts()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=entity_counts.index, y=entity_counts.values)\n",
        "        plt.title('Distribution of Detected Entity Types')\n",
        "        plt.xlabel('Entity Type')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the figure\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'entity_distribution.png'))\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No entities were extracted from the sample text.\")\n",
        "\n",
        "# Instructions for improving the model\n",
        "print(\"\"\"\n",
        "# Next Steps to Improve Your Talmud NER System:\n",
        "\n",
        "1. Manual Annotation:\n",
        "   - Use the annotation sample to manually identify and label entities\n",
        "   - Create a more comprehensive training dataset\n",
        "\n",
        "2. Use Pretrained Transformers:\n",
        "   - For better performance, consider fine-tuning a transformer model like BERT\n",
        "\n",
        "3. Add Gazetteer Lists:\n",
        "   - Create lists of known Talmudic names, places, and titles\n",
        "   - Incorporate these into rule-based components to augment the ML approach\n",
        "\n",
        "4. Handling Hebrew/Aramaic:\n",
        "   - If working with original texts, use a multilingual model or specialized Hebrew NLP tools\n",
        "\n",
        "5. Active Learning:\n",
        "   - Implement a workflow where the model suggests entities and you confirm/correct them\n",
        "   - Use this to iteratively improve the model with minimal annotation effort\n",
        "\n",
        "6. Advanced Analysis:\n",
        "   - Once you have sufficient entities extracted, perform network analysis to discover relationships between sages\n",
        "   - Create visualizations of entity co-occurrences"
      ]
    }
  ]
}